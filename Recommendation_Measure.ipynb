{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nWaJ45dzWnVh",
        "tbXO6PEMWvzm",
        "13l2mbUgW3e6",
        "W-EK2qp-XBwq"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Установка нужных версий для работы**"
      ],
      "metadata": {
        "id": "Lm7jrXSrWcpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install polars==0.20.19 scipy==1.13.0 scikit-learn==1.4.1.post1 numpy==1.26.4 pandas==2.0.3 rapidfuzz~=3.8.1"
      ],
      "metadata": {
        "id": "vxGx7INgTX5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c236c8bf-7c8e-480c-c0f0-c8fba2cce85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting polars==0.20.19\n",
            "  Downloading polars-0.20.19-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.13.0\n",
            "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.4.1.post1\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting rapidfuzz~=3.8.1\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.1.post1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.1.post1) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Installing collected packages: rapidfuzz, polars, numpy, scipy, scikit-learn\n",
            "  Attempting uninstall: polars\n",
            "    Found existing installation: polars 0.20.2\n",
            "    Uninstalling polars-0.20.2:\n",
            "      Successfully uninstalled polars-0.20.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed numpy-1.26.4 polars-0.20.19 rapidfuzz-3.8.1 scikit-learn-1.4.1.post1 scipy-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Импорты**"
      ],
      "metadata": {
        "id": "nWaJ45dzWnVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJPOf0w9L_8C"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from rapidfuzz import process\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Директория с датасетом**"
      ],
      "metadata": {
        "id": "5oO25D3yeUbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PrMzj6FfUW32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7dee88-4443-4134-add0-0eac25ffacc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = 'drive/MyDrive/dataset'"
      ],
      "metadata": {
        "id": "ZTtwBqYseYKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Простой алгоритм**"
      ],
      "metadata": {
        "id": "tbXO6PEMWvzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_rating(count, avg, quantile: pl.DataFrame, mean: pl.DataFrame):\n",
        "    quantile = float(quantile.head(1).row(0)[0])\n",
        "    return (count / (count + quantile) * avg) + (quantile / (quantile + count) * mean)"
      ],
      "metadata": {
        "id": "U9fDoKxVMSYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_simple_recs(movies_metadata: pl.DataFrame, head_count: int) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Функция, возвращающие общие рекомендации по фильмам, без уточняющих характеристик.\n",
        "    :param movies_metadata:\n",
        "    :param head_count: Количество фильмов, которые надо возвратить в итоговом дата фрейме\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Получаем среднее значение оценок под фильмами\n",
        "    mean: pl.DataFrame = movies_metadata.select('vote_average').mean()\n",
        "    # Агрегируем столбцы этого DataFrame до квантиля 0.90.\n",
        "    quantile: pl.DataFrame = movies_metadata.select('vote_count').quantile(0.90)\n",
        "    # Получаем дата фрейм из значений количества голосов больше квантиля\n",
        "    q_movies: pl.DataFrame = movies_metadata.filter(pl.col('vote_count') >= quantile)\n",
        "    # Дописываем столбец со взвешенным рейтингом, название столбца - 'score'\n",
        "    q_movies: pl.DataFrame = q_movies.with_columns(\n",
        "        weighted_rating(\n",
        "            pl.col('vote_count'),\n",
        "            pl.col('vote_average'),\n",
        "            quantile,\n",
        "            mean\n",
        "        ).alias('Оценка алгоритма'),\n",
        "        pl.col('title').alias('Название'),\n",
        "        pl.col('vote_count').alias('Количество оценок'),\n",
        "        pl.col('vote_average').alias('Средняя оценка')\n",
        "    )\n",
        "    # Сортируем дата фрейм по убыванию взвешенного рейтинга\n",
        "    q_movies: pl.DataFrame = q_movies.sort('Оценка алгоритма', descending=True)\n",
        "    return q_movies[['Название', 'Количество оценок', 'Средняя оценка', 'Оценка алгоритма']].head(head_count)"
      ],
      "metadata": {
        "id": "YC4UmeJPMriU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **По содержанию**"
      ],
      "metadata": {
        "id": "hG5B7c1FWz9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "13l2mbUgW3e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_tfidf_recs(movies_metadata: pl.DataFrame, title: str) -> pl.DataFrame:\n",
        "    tfidf: TfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
        "    movies_metadata: pl.DataFrame = movies_metadata.with_columns(\n",
        "        pl.col('overview').fill_null('')\n",
        "    ).drop('vote_average', 'vote_count')\n",
        "    overview_series: pl.Series = movies_metadata.select('overview').to_series()\n",
        "\n",
        "    # Составляем матрицу TF-IDF\n",
        "    from scipy.sparse import csr_matrix\n",
        "    tfidf_matrix: csr_matrix = tfidf.fit_transform(overview_series)\n",
        "    cosine_sim: np.ndarray = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "    movies_metadata = movies_metadata.with_row_index(\"index\")\n",
        "    # Получаем индекс фильма, название которого совпадает с заданным\n",
        "    expr: pl.Expr = pl.all_horizontal(\n",
        "        pl.col('title') == title\n",
        "    )\n",
        "    idx = movies_metadata.row(by_predicate=expr, named=True)['index']\n",
        "    # Получаем попарную схожесть всех фильмов с фильмом, который нам дан\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    # Сортируем фильмы на основании очков схожести\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Получаем очки для 10 самых похожих фильмов\n",
        "    sim_scores = sim_scores[1:11]\n",
        "\n",
        "    # Получаем индексы фильмов\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return movies_metadata.select('title')[movie_indices]"
      ],
      "metadata": {
        "id": "L5VvxhIPMt5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# По ключевым словам"
      ],
      "metadata": {
        "id": "W-EK2qp-XBwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_director(x):\n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def get_list(x):\n",
        "    if isinstance(x, list):\n",
        "        names = [i['name'] for i in x]\n",
        "        if len(names) > 3:\n",
        "            names = names[:3]\n",
        "        return names\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def clean_data(x):\n",
        "    if isinstance(x, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
        "    else:\n",
        "        # Проверяем что директор существует. Если нет, возвращаем пустую строку\n",
        "        if isinstance(x, str):\n",
        "            return str.lower(x.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "\n",
        "def get_recommendations(title, metadata: pd.DataFrame, indices, cosine_sim):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return metadata['title'].iloc[movie_indices]\n",
        "\n",
        "\n",
        "def create_soup(x):\n",
        "    return ' '.join(str(x['keywords'])) + ' ' + ' '.join(str(x['cast'])) + ' ' + str(x['director']) + ' ' + ' '.join(str(x['genres']))\n",
        "\n",
        "\n",
        "def get_content_keywords_recs(\n",
        "        title: str,\n",
        "        movies_df,\n",
        "        credits,\n",
        "        keywords\n",
        ") -> pd.DataFrame:\n",
        "    metadata = movies_df\n",
        "    # Сломанные записи\n",
        "    if len(metadata) > 33000:\n",
        "      metadata = metadata.drop(35587)\n",
        "    if len(metadata) > 25000:\n",
        "      metadata = metadata.drop(29503)\n",
        "    if len(metadata) > 15000:\n",
        "      metadata = metadata.drop(19730)\n",
        "    keywords['id'] = keywords['id'].astype('int')\n",
        "    credits['id'] = credits['id'].astype('int')\n",
        "    metadata['id'] = metadata['id'].astype('int')\n",
        "    metadata = metadata.merge(credits, on='id')\n",
        "    metadata = metadata.merge(keywords, on='id')\n",
        "\n",
        "    features = ['cast', 'crew', 'keywords', 'genres']\n",
        "\n",
        "    for feature in features:\n",
        "        metadata[feature] = metadata[feature].apply(literal_eval)\n",
        "    metadata['director'] = metadata['crew'].apply(get_director)\n",
        "    for feature in features:\n",
        "        metadata[feature] = metadata[feature].apply(get_list)\n",
        "    for feature in features:\n",
        "        metadata[feature] = metadata[feature].apply(clean_data)\n",
        "    metadata['soup'] = metadata.apply(create_soup, axis=1)\n",
        "    count = CountVectorizer(stop_words='english')\n",
        "    count_matrix = count.fit_transform(metadata['soup'])\n",
        "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
        "    metadata = metadata.reset_index()\n",
        "    indices = pd.Series(metadata.index, index=metadata['title'])\n",
        "    return get_recommendations(title, metadata, indices, cosine_sim)"
      ],
      "metadata": {
        "id": "vdsY72G7NjgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Коллаборативная фильтрация**"
      ],
      "metadata": {
        "id": "D8fAdCJNdOj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K ближайших соседей"
      ],
      "metadata": {
        "id": "oI7ec4Pp-sHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_collaborative_knn_recs(movie_names: pd.DataFrame, ratings_data: pd.DataFrame, movie_name: str):\n",
        "    # Дата фрейм с названием фильма и его жанром\n",
        "    movie_names = movie_names[['title', 'genres']]\n",
        "    # Дата фрейм, в котором\n",
        "    movies_users: pd.DataFrame = ratings_data.pivot(index=['userId'], columns=['movieId'], values='rating').fillna(0)\n",
        "    # Преобразовываем в разреженную матрицу (CSR)\n",
        "    mat_movies_users: csr_matrix = csr_matrix(movies_users.values)\n",
        "    model_knn: NearestNeighbors = NearestNeighbors(metric='cosine', algorithm='auto', n_neighbors=30, n_jobs=-1)\n",
        "    model_knn.fit(mat_movies_users)\n",
        "    movie_index: int = process.extractOne(movie_name, movie_names['title'])[2]\n",
        "    distances, indices = model_knn.kneighbors(mat_movies_users[movie_index], n_neighbors=20)\n",
        "    recc_movie_indices: list = sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())),\n",
        "                                      key=lambda x: x[1])[:0:-1]\n",
        "    # Список с рекомендациями\n",
        "    recommend_list = []\n",
        "    # На каждый индекс рекомендаций\n",
        "    for val in recc_movie_indices:\n",
        "        # Добавляем в датафрейм рекомендаций названий фильма и расстояние\n",
        "        recommend_list.append({'Title': movie_names['title'][val[0]], 'Distance': val[1]})\n",
        "    # Датафрейм с рекомендациями\n",
        "    df = pd.DataFrame(recommend_list, index=range(1, 20))\n",
        "    return df"
      ],
      "metadata": {
        "id": "k1lyHcQVdVkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVD"
      ],
      "metadata": {
        "id": "CS8QlHQD-xHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_collaborative_svd_recs(user_id: int, num_recommendations: int, movies_df, ratings_df):\n",
        "    # Merge the two datasets\n",
        "    #movies_df.index = movies_df['movieId']\n",
        "    #ratings_df.index = ratings_df['movieId']\n",
        "    df: pd.DataFrame = pd.merge(ratings_df, movies_df, on=\"movieId\")\n",
        "    pivot_table: pd.DataFrame = df.pivot_table(index=\"userId\", columns=\"title\", values=\"rating\")\n",
        "    pivot_table = pivot_table.dropna(axis='columns', thresh=2)\n",
        "    overall_mean = pivot_table.mean(axis=1)\n",
        "    # Fill missing values with the mean rating\n",
        "    pivot_table.fillna(overall_mean, axis='index', inplace=True)\n",
        "    from scipy.sparse.linalg import svds\n",
        "    pivot_np = pivot_table.to_numpy(na_value=2.5)\n",
        "    U, sigma, Vt = svds(pivot_np, k=1)\n",
        "    user_item_matrix = sigma * Vt.T\n",
        "    user_rating_vector = user_item_matrix[user_id - 1]\n",
        "    similarity_scores = np.corrcoef(user_rating_vector, user_item_matrix)[0, 1:]\n",
        "    top_movies = np.argsort(similarity_scores)[::-1][:num_recommendations]\n",
        "    return movies_df.iloc[top_movies][\"title\"]"
      ],
      "metadata": {
        "id": "p6G722eQ-z-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Песочница**"
      ],
      "metadata": {
        "id": "U4PurAp4XbF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit"
      ],
      "metadata": {
        "id": "nqoY_ado_Yid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df: pl.DataFrame = (pl.read_csv(f'{DATASET_DIR}/movies_metadata.csv', infer_schema_length=100000)\n",
        "                               .select(pl.col(\"id\", \"title\", \"overview\", \"vote_average\", \"vote_count\", \"genres\"))).head(30000)"
      ],
      "metadata": {
        "id": "a0r4TCU2N400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ratings_df: pl.DataFrame = (pl.read_csv(f'{DATASET_DIR}/ratings_small.csv')\n",
        "                                     .select(pl.col(\"userId\", \"movieId\", \"rating\"))).head(80000)"
      ],
      "metadata": {
        "id": "TVEU0zzUPr45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Время работы простейшего алгоритма---\")\n",
        "\n",
        "times = 4000\n",
        "time = timeit.timeit(lambda: get_content_simple_recs(movies_df, 10), number=times)\n",
        "print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_simple_recs(movies_df.head(25000), 10), number=times)\n",
        "print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_simple_recs(movies_df.head(20000), 10), number=times)\n",
        "print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_simple_recs(movies_df.head(15000), 10), number=times)\n",
        "print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_simple_recs(movies_df.head(10000), 10), number=times)\n",
        "print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_simple_recs(movies_df.head(5000), 10), number=times)\n",
        "print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PastYofVP25_",
        "outputId": "b71e1991-3fdc-4189-8fdf-4b822d64d24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Время работы простейшего алгоритма---\n",
            "0.003563161302499992  секунд ушло в среднем на 1 из  4000  прогонов\n",
            "14.252645209999969  секунд всего ушло на  4000  прогонов\n",
            "0.0031699928517499956  секунд ушло в среднем на 1 из  4000  прогонов\n",
            "12.679971406999982  секунд всего ушло на  4000  прогонов\n",
            "0.0027842177917500096  секунд ушло в среднем на 1 из  4000  прогонов\n",
            "11.136871167000038  секунд всего ушло на  4000  прогонов\n",
            "0.002161087751750003  секунд ушло в среднем на 1 из  4000  прогонов\n",
            "8.644351007000012  секунд всего ушло на  4000  прогонов\n",
            "0.0014267223905000037  секунд ушло в среднем на 1 из  4000  прогонов\n",
            "5.706889562000015  секунд всего ушло на  4000  прогонов\n",
            "0.0015273430755000134  секунд ушло в среднем на 1 из  4000  прогонов\n",
            "6.109372302000054  секунд всего ушло на  4000  прогонов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Время работы алгоритма на базе TF-IDF---\")\n",
        "times = 2\n",
        "time = timeit.timeit(lambda: get_content_tfidf_recs(movies_df.head(30000), 'Jumanji').head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов с размером датасета\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_tfidf_recs(movies_df.head(25000), 'Jumanji').head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_tfidf_recs(movies_df.head(20000), 'Jumanji').head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_tfidf_recs(movies_df.head(15000), 'Jumanji').head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_tfidf_recs(movies_df.head(10000), 'Jumanji').head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_tfidf_recs(movies_df.head(5000), 'Jumanji').head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "#time = timeit.timeit(lambda: get_content_simple_recs(movies_df.head(5000), 10), number=times)\n",
        "#print(time/times, \" секунд ушло в среднем на 1 из \", times, \" прогонов\")\n",
        "#print(time, \" секунд всего ушло на \", times, \" прогонов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC4sAdO3QEMe",
        "outputId": "aa32103f-2bd3-4673-8222-87ae52872602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Время работы алгоритма на базе TF-IDF---\n",
            "41.139004016999934  секунд всего ушло на  2  прогонов с размером датасета\n",
            "24.820393908999904  секунд всего ушло на  2  прогонов\n",
            "17.50840908100008  секунд всего ушло на  2  прогонов\n",
            "10.353564688000006  секунд всего ушло на  2  прогонов\n",
            "4.459642247999909  секунд всего ушло на  2  прогонов\n",
            "1.4369325990001016  секунд всего ушло на  2  прогонов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Время работы алгоритма на базе заготовленных ключевых слов---\")\n",
        "times = 4\n",
        "movies_pd = movies_df.to_pandas()\n",
        "credits = pd.read_csv(f'{DATASET_DIR}/credits.csv', nrows=15000)\n",
        "keywords = pd.read_csv(f'{DATASET_DIR}/keywords.csv')\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_keywords_recs('Jumanji', movies_pd.head(30000), credits, keywords).head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_keywords_recs('Jumanji', movies_pd.head(25000), credits, keywords).head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_keywords_recs('Jumanji', movies_pd.head(20000), credits, keywords).head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_keywords_recs('Jumanji', movies_pd.head(15000), credits, keywords).head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_keywords_recs('Jumanji', movies_pd.head(10000), credits, keywords).head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_content_keywords_recs('Jumanji', movies_pd.head(5000), credits, keywords).head(10), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xr8EH4NQIwC",
        "outputId": "0cf72813-bcc8-46ea-f62f-527d713852e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Время работы алгоритма на базе заготовленных ключевых слов---\n",
            "85.72211765100019  секунд всего ушло на  4  прогонов\n",
            "89.46958638199976  секунд всего ушло на  4  прогонов\n",
            "82.16336677299932  секунд всего ушло на  4  прогонов\n",
            "80.80789647300026  секунд всего ушло на  4  прогонов\n",
            "56.522462799000095  секунд всего ушло на  4  прогонов\n",
            "29.150475607999397  секунд всего ушло на  4  прогонов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Время работы алгоритма KNN ---\")\n",
        "movies_pd = movies_df.to_pandas()\n",
        "user_ratings_pd = user_ratings_df.to_pandas()\n",
        "times = 100\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_knn_recs(movies_pd.head(30000), user_ratings_pd, 'Jumanji').iloc[::-1], number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_knn_recs(movies_pd.head(25000), user_ratings_pd, 'Jumanji').iloc[::-1], number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_knn_recs(movies_pd.head(20000), user_ratings_pd, 'Jumanji').iloc[::-1], number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_knn_recs(movies_pd.head(15000), user_ratings_pd, 'Jumanji').iloc[::-1], number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_knn_recs(movies_pd.head(10000), user_ratings_pd, 'Jumanji').iloc[::-1], number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_knn_recs(movies_pd.head(5000), user_ratings_pd, 'Jumanji').iloc[::-1], number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94MNjCHyY3-g",
        "outputId": "14e94b60-12dd-4896-fe3e-212097648ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Время работы алгоритма KNN ---\n",
            "13.619223024999883  секунд всего ушло на  100  прогонов\n",
            "13.17771265600004  секунд всего ушло на  100  прогонов\n",
            "13.02451474999998  секунд всего ушло на  100  прогонов\n",
            "13.228877860000466  секунд всего ушло на  100  прогонов\n",
            "13.447778724000273  секунд всего ушло на  100  прогонов\n",
            "13.305383921000612  секунд всего ушло на  100  прогонов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Время работы алгоритма SVD ---\")\n",
        "times = 100\n",
        "movies_df: pd.DataFrame = pd.read_csv(f'{DATASET_DIR}/movies_metadata.csv', low_memory=True, encoding='latin-1')\n",
        "movies_df.drop(columns=['budget'])\n",
        "movies_df = movies_df.drop([19730, 29503]).head(30000)\n",
        "movies_df.rename(columns={'id': 'movieId'}, inplace=True)\n",
        "ratings_df: pd.DataFrame = pd.read_csv(f'{DATASET_DIR}/ratings.csv', encoding='latin-1').head(80000)\n",
        "ratings_df['movieId'] = ratings_df['movieId'].astype(int)\n",
        "movies_df['movieId'] = movies_df['movieId'].astype(int)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_svd_recs(1, 20, movies_df, ratings_df), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_svd_recs(1, 20, movies_df.head(25000), ratings_df), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_svd_recs(1, 20, movies_df.head(20000), ratings_df), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_svd_recs(1, 20, movies_df.head(15000), ratings_df), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_svd_recs(1, 20, movies_df.head(10000), ratings_df), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")\n",
        "\n",
        "time = timeit.timeit(lambda: get_collaborative_svd_recs(1, 20, movies_df.head(5000), ratings_df), number=times)\n",
        "print(time, \" секунд всего ушло на \", times, \" прогонов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp8N8uIy_oyS",
        "outputId": "031f1a98-eeb2-463b-d3b2-30d65b0d4768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Время работы алгоритма SVD ---\n",
            "29.31651280299957  секунд всего ушло на  100  прогонов\n",
            "21.2284621889994  секунд всего ушло на  100  прогонов\n",
            "22.004887082001005  секунд всего ушло на  100  прогонов\n",
            "19.33256030300072  секунд всего ушло на  100  прогонов\n",
            "15.2411935300006  секунд всего ушло на  100  прогонов\n",
            "11.502154998999686  секунд всего ушло на  100  прогонов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdf7cF2ZMAfu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}